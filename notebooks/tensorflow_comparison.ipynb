{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: TensorFlow Baseline Comparison\n",
    "\n",
    "This section implements the exact same network architectures using TensorFlow/Keras and compares:\n",
    "1. **Ease of Implementation**: Code complexity and development time\n",
    "2. **Training Performance**: Speed and convergence behavior  \n",
    "3. **Final Results**: Accuracy and loss values\n",
    "\n",
    "We'll implement:\n",
    "- XOR problem with 2-4-1 architecture\n",
    "- MNIST autoencoder with identical encoder-decoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": nu
   "metadata": {},
   "outputs": [],
   "source":[
    "# Cell 1: Import required libra",
    "import sys\n",
    "import os\n",
    "sys.path.appen,
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#,
    "\n",
    ",
    "from lib.U\n",
    "from lib.losses import MeanSquaredError\n",
    "from lib.optim",
    "\n",
    "print(\"✓ Imported custom\n",
    "\n",
    "# Try to load existing",
    "try:\n",
    "    with open('../autoe",
    "        custom_ae
    "    print(\"✓ L",
    "exce\n",
    "    print(\"⚠ Custom autoencoder results not
    "    cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outpu: [],
   "source": [
    "# Cell 2: TensorFlow Imports and Setup\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import 
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    \n",
    
   
    "# Set random seedsty\n",
    "tf.random.set_seed(42)
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "sourc
    "# Cell 3: XOR Data Setup\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xon",
    "\n",
    "print(\"XOR Dataset for co",
    "print(\"Inputs:\", n",
    n",
    \n",
   
   ]
  },
  {
   "cell_type": "
   "execution_": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce",
    "def train_custom_xor():\n",
    "    \"\"\"Train XOR using our custom ne
    "    print(\"Training XOR with Custom \n",
    
     \n",
   \n",
    "    \n",
    "    # Create network w\n",
    "    network =k()\n",
    "    network.\n",
    "    netwo",
    "    network.add(Dense(4, 1))  # Output laye\n",
    "    network.add(Sigmoid())    #\n",
    "    \n",
    "    # Set up loss and optimizer\n",
    "    loss_fn = MeanSquared
    "    opti\n",
    "    \n",
    "    prin
    "    print(\"  Input: 2 neurons\")\n",
    "    print(\"  Hidden: 4 neurons (Ta
    "    print(\"  Output: 1 neuron (Sigmoid)\")\n",
    "    print(\"  Loss: Mean Squared Error\")\n",
    "    print(
    "    \n",
    "    # Training loop\n",
    "    epochs = 2000\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"\\nTraining fo
    "    \n",
    "    for \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        
    "        for i in range(le,
    "            # Forward pass\n",
    "            x = X_xor[i:i",
    "            y_true = y_x",
    "            \n",
    "            y_pred = net,
    "         \n",
    "        s\n",
    "            loss = loss_fn.forward(y_pred, y_tru
    "        ,
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_pred, y_tr
    "            network.backward(grad)\n",
    "        n",
    "            # Update weights\n",
    "            optimizer.update(network.layers)\n",
    "        \n",
    "        
    "        avg_loss",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 500 == 0:\n"
    "            print(f\"  Epoch {epon",
    "    \n",
    "    train,
    "    \n",
    "    # Final predictions\n",
    "    final_predictions = []\n",
    
    ",
   \n",
    "    \n",
    "    final_predictions \n",
    "    final_los,
    "    accuracy",
    "    \n",
    "    print(f\"\\nTraining completed i",
    "    print(f\"Final loss: {final_loss:.
    "    print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': fin",
    "        \n",
    "        'training_time': training_time,\",
    "        'final_loss': fin
    "        'accuracy': accuracy,\n",
    "        'network': networ
    "    }\n",
    "\n",
    "# Train custom XOR\n",
    "custom_xor_results = train_custom_xor()"
   ]
  },
  {
   "cell_type": "code"
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Te
    "def train_tensorflow_xor():\n",
    "    \"\"\"Train XOR using TensorFlow/Keras with id",
    "    print(\"\\nTraining XOR with TensorFlo
    "    print(\"
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create identical network architecture: 2 -> 4 -> 1\n",
    "    mode",
    "        layers.Dense(4, activation='tanh', input
    "        layers.Dense(1, activationid\n",
    "    ])\n",
    "    \n",
    "    # Compile with identical settings to our custom implementat
    "    mode",
    "        optimizer=optimizers.SGD(learni
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    print(\"Model Architec")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train tdel\n",
    "    history = model.fit(\n",
    "        ,
    "        epochs=2000,\n",
    "        verbose=0,  # Suppress output for cleaner display\n",
    "        \n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    final_predictions = model.
    ",
    n",
   
    "    print(f\"Train
    "    print(f\"Final los,
    "    print(f\"\n",
    "    \n",
    "    retur",
    "        'predictions': final_predictions,\n",
    "        'losses': history.history['loss'],\n",
    "        'training_time': training_time,\n",
    "        ",
    "        'accuracy': accuracy,\n",
    "        'model': model\n",
    "    }\n"
    "\n",
    "# Train TensorFlow XOR\n",
    "tf_xor_results = train_tensorflow_xor()"
   ]
  },
  {
   "cell_typecode",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: XOR Results Comparison\n",
    "def compare_xor_implementations():\n",
    "    \"\"\"Compare XOR results between custom and Te",
    "    \n",
    "    prin
    "    print(\"=\" * 50)\n",
    "    
    "    print(\"\\nFinal Predictions:\")\n",
    
    n",
   
    "    \n",
    "    for i in range(len):\n",
    "        custon",
    "        tf_p
    "        \n",
    "        target = int(y_xor[i][0])\n",
    "        custom_binary = int(custom_pred > 0.5)\n",
    "        tf_binary = int(tf_pred > 0.5)\n",
    "        \n",
    "        custom_correct = \n",
    "        n",
    "        \n",
    "        n",
    "    \n",
    "    print(\"\\nImplementation Comparison:\n",
    "    print(\"Custom Li
    "    print(\"  - Manual training loop (40+ lines)\")\n",
    "    print(\"  - Explicit forward/backward passes\")\n",
    "    print(\"n",
    "    print(\"  - Sampl",
    "    \n",
    "    print(\"\\nTensorFlow/Keras:\")\n",
    "    print(",
    "    prin
    "    print(\"  - Built-in optimization and\n",
    "    print(\"  - Batch proces",
    "    \n",
    "    # Speed comparison\",
    "    custom_time = custom_xo",
    "    tf_ti\n",
    "    spee
    "    \n",
    "    print(f\"\\nPerformance:\\n",
    "    prin,
    "    print(f\"  - TensorFl
    "    print(f\"  - TensorFlow is {sp
    "    \n",
    "    # Accuracy compari",
    "    print(f\"\\nAccuracy:\
    "    print(f\"  - Custom Library: {custom_xor_",
    "    print(f\"  - TensorFlow: {tf_xor_r,
    "    \n",
    "    # Lo
    "    print(f\"\\nFinal Loss:\")\n",
    "    prin\n",
    "    print(f\"  - TensorFlow: {tf_xor",
    "\n",
    "compare_xor_implementations()"
   ]
  },
  {
   "cell_type",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: XOR Training Curves Visualization",
    "def plot_xor_training_curves():\n",
    "    \"\"\"Plot training curves for both implement
    "    \n",
    "    plt.f,
    "    ",
    "    # Training loss comparison\n",
    "    plt.subplot(1, 3, 1)\n",
    )\n",
    ,
   
    "    plt.ylabel('Lo",
    "    plt.title('XOR Tra",
    "    plt.legen",
    "    plt.grid
    "    plt.y,
    "    \n",
    "    # Training time comparison\n",
    "    plt.subplot(1, 3, 2)\n",
    "    time\n",
    "    labels = ['Custom', 'TensorFlow']\n",
    "    colo\n",
    "    \n",
    "    bars = plt.bar(labels, times, color=colors, alpha=0.7)\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.,
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{time_val:.3f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # Final accuracy comparison\n",
    "    plt.subplot(1, 3, 3)\n",
    "    accuracies = [custom_xor_results['accuracy']*100, tf_xor_results0]\n",
    "    \n",
    "    bars = plt.bar(labels, accuracies, c
    "    plt.\n",
    "    plt.title('Final Accuracy Co",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc_val in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.",
    "        
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_xor
   ]
  },
  {
   "cell_type": "code",
   "execution
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: MNIST Data Preparation for Autoencoder\n",
    "def prepare_mnist_for_comparison(max_samples=5000):\n",
    "    \"\",
    "    \n",
    "    # Load MNIST data using TensorFlow\n",
    "    (X_train_full, _), (X_test_full, _) = mnist.load_",
    "    \n",
    "    # Non",
    "    X_train_full = X_train_full.as
    "    X_test_full = X_test_full.astype('float32') / 255
    "    \n",
    "    X_train_flat = X_train_full.reshape(X_train_full.shape[0], -1)\n",
    "    X_te",
    "    \n",
    "    # Use subset for f",
    "    X_train = X_train_flat[:max_samples]\n",
    "    X_te
    "    \n",
    "    print(f\"MNIST Data for Compari",
    "    print(f\"Training data shape: {X_,
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Data range: [{X_train.min():.3f}, {X_tra",
    "    \n",
    "    return X_train, X_test",
    "\n",
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: TensorFlow Autoencoder Implementation\n",
    "def train_tensorflow_autoencoder(:\n",
    "    \"\"\"Train autoencoder using TensorFlow/Keras with identical archite,
    "    print(\"\\nTraining Autoencoder with TensorFlow/Keras\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create identical autoencoder architecture: 784 -> 128 -> 32 -> 128 -> 784\n",
    "    autoencoder = models.Sequential([\n",
    "        # Encoder\n",
    "        layers.Dense(1
    "        ",
    "        \n",
    "        # Decoder\n",
    "    ",
    "        layers.Dense(784, act\n",
    ",
    
   
    "    autoencoder.co
    "        optimizer=opti,
    "        loss=",
    "        metr
    "    )\n",
    "    \n",
    "    print(\"Model Architecture:\")\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = autoencoder",
    "        ,
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, X_test),\n",
    "        verbose=1  # Show progress\n",
    "    )\n",
    "    \n",
    "    trai\n",
    "    \n",
    "    # Final predictions and loss\n",
    "    test_predictions = autoencoder.predict(X_test, verbose=0)\n",
    "    final_test_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    prin",
    "    print(f\"Final test loss: {final_,
    "    \n",
    "    return {\n",
    "        'model': autoencoder,\n",
    "        ],\n",
    "        'test_losses': history.history['val_loss'],\,
    "        'training_time': training_time,\n",
    "        'final_test_loss': final_test_loss,\n",
    "        'test_predictions': test_predictions,\n",
    "        'hist
    "    }\n",
    "\n",
    "# Train TensorFlow autoencoder\n",
    "tf_ae_results = train_tensorflow_autoencoder(X_train_comp, X_test_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1",
    "def plot_tensorflow_comparison():\n",
    "    \"\"\"Create comparison plots between implementations.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 1\n",
    "    \n",
    "    # 1. Training curves comparison\n",
    "    axes\n",
    "    axes[0, 0].plot(tf_ae_r,
    "    \n",
    "    if custom_ae_results and 'train_losses' in custom_ae_re
    "        axes[0, 0].plot(custom_ae_results['train_los
    "                       linewidth=2, linestyle='--', color='l
    "        i
    "        \n",
    "                           linewidth=2, li",
    "    \n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes
    "    axes[0, 0].set_title('Autoencode
    "    axes[0, 0].legend()\n,
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Performance metrics\n",
    "    training_times = [tf_ae_results['training_time']]\n",
    "    labels = ['TensorFlow']\n",
    "    \n",
    "    if custom_ae_results and 'traini,
    "        training_times.append(custom_ae_results['training_time'])\n",
    "        labels.append('Custom')\n",
    "    \n",
    "    bars = axes[0, 1].bar(labels, training_times, alpha=0.8, color=['blue', 'oran
    "    ",
    "    axes[0, 1].set_title('Training S)\n",
    \n",
    ,
   
    "    for bar, time_",
    "        axes[0, 1].tex
    "             om')\n",
    "    \n",
    "    # 3. 
    "    final_losses = [tf_ae_re
    "    \n",
    "    if custom_ae_results and 'final_test_loss' in custom_a",
    "        ,
    "    \n",
    "    bars = axes[0, 2].barn",
    "    axes
    "    axes[0, 2].set_title('Final,
    "    axes[0, 2].grid(True, alpha
    "    \n",
    "    # Add value labels on bars
    "    for bar, loss_val in zip(b",
    "        001,\n",
    "                       f'{loss",
    "    \n",
    "    # 4. Reconstruction examples\n",
    "    n_examples = 5\n",
    "    indices = np.random.choice(len(X_test_images), n_examples, replace=False)\n",
    "    \n",
    "    # Original images\n",
    "    combined_original = np.hstack([X_test_images[idx] n",
    "    axes[1, 0].imshow(combined_original, cmap='gray')\n",
    "    axes[1, 0].set_title('Original MNIST Samples')\n",
    "    axes[1, 0].axis('off')\n ,
    "    \n",
    "    # TensorFlow rec\n",
    "        'Metric': [\n",
    "            'XOR Final Loss',\n",
    "            'XOR Training Time',\n",
    "            'XOR Implementation (lines)',\n",
    "            'Autoencoder Final Loss', \n",
    "            'Autoencoder Training Time',\n",
    "            'Autoencoder Implementation (lines)',\n",
    "            'Total Code Reduction',\n",
    "            'Speed Improvement',\n",
    "            'Educational Value',\n",
    "            'Production w(combined_cus",
    "        axes[1\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5n",
    "               transform=axes[1,
    "        axes[1, 2].set",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout(),
    "    plt.show()\n",
    "\n",
    "plot_tensorflow_compari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Comprehensin",
    "def comprehensive_compa",
    "    \"\"\"Provide compr
    "    \n",
    "    print(\"\\nCOMPREH\n",
    "    print(\"=)\n",
    "    \n",
    "    prin,
    "    print(\"   Custom Library:\")\n",
    "    print(\"   ✓ Requires manual implementation 
    "    prin\n",
    "    print(\"   ✓ More verbose code with explicit forward/backward passes\")\n",
    "    print(\"   ✓ Better understanding of underlying mechanics\")\n",
    "    \n",
    "    print(\"\\n   TensorFlow/Keras:\")\n",
    "    print(\"   ✓ High-level AP
    "    print(\"   ✓ Built-in training loops with)\n",
    "    print(\"   ✓ Automatic batching, shuffling, and ",
    "    print(\"   ✓ Much more concise code (~60%n",
    "    \n",
    "    print(\"\\n2. PERFORMANCE:\")\n",
    "    print\n",
    "    prinn",
    "    print(f\"   ✓ Custom library limited by Python/NumPy performance\",
    "    \n",
    "    # Ca,
    "    xor_speedup = custom_xor_results['training_time'] / tf_xor_results['training_time
    "    \n",
    "    \n",
    
    ts:\n",
   ",
    "        print(f\"   ✓ 
    "    \n",
    "    print,
    "    print(\"  \n",
    "    n",
    "    print(\"   ✓ TensorFlow may have slight advantages due to optimized initialization\")\n",
    "    
    "    print(\"\\n4. DEBUGGING & TRANSPARENCY:\")\n",
    "    print(\"   Custom Library:\")\n",
    "    print(\"   ✓ Full control over every operation\")\n",
    "    print(\"   ✓ Easy to inspect gradients and intermediate values\")\n",
    "    print(\"   ✓ Better for educational purposes\")\n",
    "    
    "    print(\"\\n   TensorFlow:\")\n",
    ,
   ",
   n",
    "    \n",
    "    print(\"
    "    \n",
    "    complexity_data
    "        'Compon,
    ",
    "        'Tensorn",
    "        'Reduction,
    "    }\n",
    "    \n",
    "
    "    print(\"\\n   Code)\n",
    "    print(df.to_string(ind
    "    \n",
    "    print(\"\\n\\nCONCLUSION:",
    "    print(\"=\" * 15)\n",
    "    print(\"The 
   
   n",
    "    print(
    "    print(\"alg,
 
: 4
}t_minor"
 "nbformaormat": 4,},
 "nbf
  }
 ": "3.8.5"on
   "versipython3",xer": "igments_le",
   "pypythonr": "xporte"nbconvert_e
   "python","name": ",
   /x-pythontext: ""mimetype"
   ".py",: xtension"file_e,
   "": 3
   }on    "versithon",
pyame": "i    "n": {
ror_modedemir
   "co: {uage_info""lang
  n3"
  },"pytho"name": ,
    "python"nguage":",
   "lathon 3me": "Pynasplay_"diec": {
   elsp {
  "kernadata":
 ],
 "met
   ]
  }learning."n deep moderr t powes thand algorithmthematics a marlyingundeinto the e insights aluabls invffer oscratchks from ural networ nenting implemeperformance,nd enience ade convoviprTensorFlow orks like mewhile frahat wonstrates t demmparison co   "Then",
 
    "\ns\n",icatiold applwor real-forter suited low betnsorF*: Teion Use*roduct**P
    "5. \n",rstandingdeeper underovides  plementationimpe**: Custom onal Valu*Educati    "4. *ns\n",
stractio abh-level through higby ~71%educes code sorFlow rity**: Tende Complex **Co3.    "\n",
kendC++ bacoptimized o  tter dueasis 2-10x fFlow *: Tensorormance**Perf * "2.
   ",esults\nble rparaom cions achievemplementatBoth iality**: on**Functi"1.    \n",
 sights In### Key  ""\n",
  
    n\n",ntatiostom impleme cutingdali results, valar simichieve: Both acy***Accura"- *\n",
    d backendo optimizeue tfaster dntly w significarFlo Tensoformance**: **Per
    "-on\n", optimizatith automaticall wiit()` c.f `model Single*:*TensorFlow*  "- *s\n",
  rd passeckward/barwafot with explicing loop anual traini**: Mmentationtom Imple "- **Cuss\n",
   sultProblem Re# XOR   "##\n",
    "
  are:\n",ngs e key findis. ThsorFlow/Kerand Tenntation alemework impural netustom netween our cison beparive comnss a comprehedeproviebook his not    "T "\n",
\n",
   ## Summary "ce": [
   
   "sour": {},ta
   "metadadown",e": "mark "cell_typ,
  {
  
  }y()"
   ]ummaral_seate_fin",
    "cr"\n
    ")\n",.pkl\ison_resultsparrflow_comd to tensoaven results sCompariso\n✓ nt(\"\   "    pri",
 "    \n,
    lts, f)\n"rison_resumpadump(co   pickle.    "      f:\n",
'wb') aslts.pkl', arison_resuflow_comp'../tensor open(  with  "  
      \n",    "
    }\n",",
    "ta\n_daarye': summry_tabl    'summa  "    \n",
  esults,tom_ae_rer': cus_autoencod     'custom   
    "sults,\n",ae_recoder': tf_utoenow_ansorfl    'te"    \n",
    results, tf_xor_rflow_xor':    'tenso"       n",
 r_results,\tom_xor': cusm_xo    'custo    ,
    "{\n"ts = rison_resul compa    " n",
  rence\refe for future e resultsav    # S  "  \n",
     "\n",
    5.\")on Sectintationoject documeed in the prfi"speci\"    print( \n",
   \")equirement rlineasesorFlow bhe Ten tn satisfiesompariso\nThis cint(\"\   "    pr  \n",
    "  ))\n",
 False(index=_stringtomary_df.nt(sum"    pri\n",
    )datame(summary_raataF.D_df = pd  summary    "   \n",
  " ",
        }\n"   \n",
    ]   "     ",
 '\n 'High           "  ,\n",
    'Medium'      "      
  ,\n",\"e']:.1f}xing_timrainresults['te']/tf_xor_g_timintrainor_results['{custom_x       f\"    "     
%',\n",  '~71         "     ',\n",
'~20         "      ",\n",
 f}s\me']:.1_ti'trainingae_results[\"{tf_     f "         \n",
 ]:.4f}\",loss'test_['final__resultsf_ae     f\"{t   
    "    n",',\   '~13       "      ",\n",
.1f}%\']*100:s['accuracylt_xor_resu{tf\"           f" \n",
    ]:.3f}s\",_time'ts['trainingor_resul\"{tf_x        f"      
  \",\n",oss']:.4f}'final_llts[f_xor_resu     f\"{t      "     ,
ow': [\n"TensorFl    '    
    "\n",  ],       "  
 'Low'\n",  "          \n",
      'High',         "   \n",
  line',ase    'B      ,
    "  ,\n"Baseline'          '"     \n",
      '~55',"       
    ",_time,\ntom_ae        cus     " ",
  _loss,\ntom_ae cus              " n",
0',\ '~6"           ",
    %\",\n*100:.1f}racy']ults['accuesustom_xor_r  f\"{c       
    "   \",\n",f}s_time']:.3ainingsults['tr_xor_retom f\"{cus           " ,
   \",\n"']:.4f}al_lossults['finor_res_xf\"{custom        "     n",
    [\ibrary':om L   'Cust"      ,\n",
         ],
    "  '\n"on ReadinessProducti       '"        
 \n",nal Value',  'Educatio           "  ",
 \nR)',ent (XOprovem'Speed Im            ,
    "on',\n"Code Reducti'Total           
    "  es)',\n",ntation (linImplemeder   'Autoenco         " 
    ",\n',g Timeoder Trainin   'Autoenc           "  \n",
al Loss', der Fin 'Autoenco          
    " ',\n",ines)ion (lImplementat    'XOR       ,
    "  y',\n"'XOR Accurac           
    " ime',\n",ning TR Trai      'XO  "      
  n",nal Loss',\'XOR Fi          ",
    "  [\n'Metric': "        ",
    _data = {\nary    summ
    "  \n","   
   "\n",s\ime']:.1f}['training_tresults"{custom_ae_\_time = f   custom_ae        ",
    " ts:\nsulustom_ae_reme' in c'training_ti       if    " n",
 :.4f}\"\oss']final_test_lae_results['\"{custom__loss = fustom_ae          c "  
   ",\n:resultsstom_ae_n cuoss' it_ltesnal_     if 'fi"   ",
    lts:\n_ae_resu if custom   "    \n",
 
    "   ", = 'N/A'\ntimecustom_ae_  "    \n",
  = 'N/A'ae_loss   custom_ "  
   sults\n", retualGather ac    # 
    "  \n",   "  )\n",
 \" * 40\"=t("    prin    )\n",
"SUMMARY\MPARISON AL CO\\nFINt(\""    prin\n",
        "",
    \n"\".\"\ummary tableison sal compareate fin"\"\"Cr  \    "  \n",
ry():ummafinal_seate_ cr   "defn",
 able\ry Tl Summa 12: Fina Cell
    "#source": [[],
   "s": output   "": {},
data"metal,
   ": nulion_count   "execute",
pe": "cod_ty   "cell

  },
  {  ]()"
 lysis_anarisone_compaprehensiv "comn",
   "\ \n",
   orks\")l framewgh-leve value of hitheights arison highle comp✓ Th  print(\""  ",
    )\npment\"g and develo in learnin their placeaveoaches hh apprt(\"✓ Bot "    prin\n",
   nience\")nvend corformance aeady peoduction-rprovides prow orFl(\"✓ Tens  print  "  ",
  \n\")e conceptsding of corstanes undervalidaton atiimplementom "✓ Cust print(\  ,
    " n"\eaways:\") TaknKey(\"\\    print    "